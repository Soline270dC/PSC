{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"data\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 100\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 1\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 50\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, root = dataroot):\n",
    "        self.root = root\n",
    "        self.dataset = self.build_dataset()\n",
    "        self.length = self.dataset.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        step = self.dataset[:, idx]\n",
    "        # step = torch.unsqueeze(step, 0)\n",
    "        target = 0  # only one class\n",
    "        return step, target\n",
    "\n",
    "    def build_dataset(self):\n",
    "        dataset = np.load(os.path.join(self.root, \"round1.npy\")).T\n",
    "        dataset = torch.from_numpy(dataset).float()\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.5400,  4.0700,  8.8200,  ...,  4.5400,  4.3200,  4.6000],\n",
      "        [ 6.6100,  6.0400,  9.0800,  ...,  7.9800,  4.7800,  5.0100],\n",
      "        [10.3900,  6.1200, 12.5400,  ...,  8.1800,  9.3700, 11.6600],\n",
      "        [ 5.7700,  6.3400,  3.9200,  ...,  7.2800,  0.0800,  7.0900]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b0b2221e80>]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Dataset(dataroot)\n",
    "print(dataset.dataset)\n",
    "plt.plot(dataset.dataset[:, 0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "# dataset = np.load(\"data/round1.npy\")\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 6.0700,  9.0900,  7.5900,  2.4500],\n",
       "         [ 5.9500,  6.6400, 13.7800,  2.1100],\n",
       "         [ 5.2800,  8.5400, 10.4000,  3.6900],\n",
       "         [ 4.1800,  7.0600, 10.8900,  4.0500],\n",
       "         [ 4.5700,  6.2400,  9.0600,  2.0500],\n",
       "         [ 9.1100,  5.4700,  9.5800,  1.0300],\n",
       "         [ 4.9000,  4.8400,  6.0300,  1.5700],\n",
       "         [ 6.1200,  6.5700, 13.4300,  4.0000],\n",
       "         [ 5.3800,  6.5200,  8.9300,  1.7700],\n",
       "         [ 5.9700,  9.7000,  9.9700,  4.9400],\n",
       "         [ 4.8900,  3.6100,  9.4400,  2.4200],\n",
       "         [ 4.1100,  5.8500, 11.3100,  3.5500],\n",
       "         [ 5.6400,  3.4100, 10.0600,  1.4600],\n",
       "         [ 4.1000,  3.6000,  6.4300,  1.0800],\n",
       "         [ 5.9500,  5.1400, 10.0100,  1.7400],\n",
       "         [ 6.8400,  8.9300, 12.9000,  5.5100],\n",
       "         [ 4.8600,  4.8700, 11.0000,  0.0000],\n",
       "         [ 4.9400,  6.3100, 10.7200,  2.0600],\n",
       "         [ 9.3100,  7.0400, 11.7800,  3.6800],\n",
       "         [ 5.6200,  9.4200,  9.7700,  5.3500],\n",
       "         [ 5.1400,  5.7600, 12.1900,  6.3800],\n",
       "         [ 6.4700,  5.7100,  7.8300,  0.8200],\n",
       "         [ 4.2200,  7.7700, 13.5500,  1.2300],\n",
       "         [ 5.2300,  6.2100,  9.7800,  7.0300],\n",
       "         [ 7.7600,  6.9100, 10.6100,  2.9100],\n",
       "         [ 5.7600,  6.3400, 11.3100,  4.9500],\n",
       "         [ 4.0100,  4.8100, 11.9500,  2.9600],\n",
       "         [ 5.4600,  5.1300, 11.2700,  1.8600],\n",
       "         [ 3.8500,  3.8600,  5.7900,  2.4300],\n",
       "         [ 4.3000,  9.9500,  9.4000,  3.2400],\n",
       "         [ 5.3600,  4.1600,  5.5200,  2.8300],\n",
       "         [ 3.0000,  6.8700,  7.6100,  4.6100],\n",
       "         [ 4.9200,  8.3200,  9.1600,  2.2200],\n",
       "         [ 4.1900,  6.9600,  7.6400,  4.6100],\n",
       "         [ 4.3000,  7.5000,  9.1600,  3.4400],\n",
       "         [ 4.5100,  5.3900, 10.9100,  2.9100],\n",
       "         [ 5.9100,  7.7200, 11.7100,  3.1100],\n",
       "         [ 3.8600,  4.9100,  9.0600,  0.5900],\n",
       "         [ 3.9400,  6.8700,  9.3800,  4.4000],\n",
       "         [ 3.8600,  6.3300,  8.1500,  3.9100],\n",
       "         [ 3.5300,  5.9300,  8.4600,  3.2300],\n",
       "         [ 6.1000,  3.6200,  9.2400,  5.2500],\n",
       "         [ 5.5000,  7.7200, 11.3900,  3.4100],\n",
       "         [ 4.8000,  5.7800, 11.9000,  2.3000],\n",
       "         [ 3.0700,  7.7700,  9.6900,  8.1700],\n",
       "         [ 4.8300,  5.1600,  9.7400,  2.0000],\n",
       "         [ 4.9100,  5.1200, 11.7900,  0.0000],\n",
       "         [ 4.2600,  4.8100,  7.4400,  3.4300],\n",
       "         [ 5.7700,  9.5000, 15.1600,  8.7800],\n",
       "         [ 5.0900,  3.3300, 10.3800,  3.8800],\n",
       "         [ 6.0400,  9.7400, 11.9600,  5.8600],\n",
       "         [ 6.1700,  3.5600,  9.3800,  3.0900],\n",
       "         [ 5.3700,  6.2900,  9.6500,  4.7100],\n",
       "         [ 5.1600,  7.0200, 12.8100,  3.3500],\n",
       "         [ 5.3000,  4.9000,  9.2000,  1.4600],\n",
       "         [ 5.5200,  6.3400,  7.4200,  1.5400],\n",
       "         [ 6.1800,  8.1200, 11.3800,  3.8100],\n",
       "         [ 4.9000,  8.0200,  8.5400,  3.2200],\n",
       "         [ 4.2400,  3.8000,  8.4300,  1.9800],\n",
       "         [ 5.6100,  4.6600, 14.2500,  4.5300],\n",
       "         [ 9.6200, 10.5900, 11.1200,  4.5800],\n",
       "         [ 5.9300,  9.3200, 10.2200,  4.8900],\n",
       "         [ 4.0900,  5.7400, 10.9600,  2.7700],\n",
       "         [ 3.9800,  8.4600, 10.3300,  2.3700],\n",
       "         [ 5.9100,  6.1900,  9.1100,  4.5400],\n",
       "         [ 4.4500,  4.7200,  9.1200,  3.9800],\n",
       "         [ 4.6800,  4.7400, 10.1200,  1.7200],\n",
       "         [ 5.3300,  6.0000, 10.7200,  2.9900],\n",
       "         [ 5.6000,  3.9800, 10.1900,  1.6800],\n",
       "         [ 6.8000,  7.0200,  8.8100,  6.0100],\n",
       "         [ 6.2300,  7.0100,  9.4500,  1.8100],\n",
       "         [ 6.1100,  9.6100,  8.7100,  2.6100],\n",
       "         [ 4.4300,  5.6000,  9.3300,  5.7400],\n",
       "         [ 4.4200,  5.0100, 11.4100,  2.6800],\n",
       "         [ 4.6200,  6.5600,  7.3900,  0.9600],\n",
       "         [ 4.6600,  4.3100,  5.7500,  4.2300],\n",
       "         [ 8.6700, 10.3200, 10.7500,  4.8600],\n",
       "         [ 5.8000,  4.5400,  8.2600,  1.7200],\n",
       "         [ 5.1000,  6.5000,  8.8300,  3.0200],\n",
       "         [ 5.6000,  7.4500, 11.2900,  3.8200],\n",
       "         [ 4.4000,  4.7100,  8.6200,  4.2200],\n",
       "         [ 5.9300,  6.6600, 13.2600,  1.2100],\n",
       "         [ 5.7900,  7.9000,  9.7100,  4.8700],\n",
       "         [ 5.5900,  8.5800,  7.9500,  4.4900],\n",
       "         [ 4.3500,  9.7100, 11.5900,  5.6200],\n",
       "         [ 4.2700,  4.1500,  7.3600,  1.0300],\n",
       "         [ 6.5200,  7.6600,  7.2400,  3.2700],\n",
       "         [ 5.6900,  8.4000,  9.1000,  4.7300],\n",
       "         [ 4.5100,  8.8500, 11.0200,  6.7000],\n",
       "         [ 4.0900,  7.1100,  8.7600,  3.2700],\n",
       "         [ 3.8000,  6.3700, 10.8400,  1.0800],\n",
       "         [ 4.4900,  6.3300, 12.8700,  1.1600],\n",
       "         [ 5.8000,  5.1800,  8.6100,  0.0000],\n",
       "         [ 4.6800,  4.4400,  8.9400,  4.3700],\n",
       "         [ 6.0100,  4.4500,  6.2300,  2.1800],\n",
       "         [ 5.0100,  3.7800,  9.0200,  3.0400],\n",
       "         [ 6.1700,  6.4300, 12.3600,  3.2000],\n",
       "         [ 7.3300,  4.3000, 11.2900,  3.5100],\n",
       "         [ 7.3100,  5.0000, 11.2400,  3.7300],\n",
       "         [ 3.7300,  6.5100,  8.7300,  3.1600]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0])]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on ``netG`` and ``netD``\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose1d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm1d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*8) x 4 x 4``\n",
    "            nn.ConvTranspose1d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*4) x 8 x 8``\n",
    "            nn.ConvTranspose1d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*2) x 16 x 16``\n",
    "            nn.ConvTranspose1d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf) x 32 x 32``\n",
    "            nn.ConvTranspose1d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. ``(nc) x 64 x 64``\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose1d(50, 512, kernel_size=(4,), stride=(1,), bias=False)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose1d(512, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose1d(256, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose1d(64, 1, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-GPU if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly initialize all weights\n",
    "#  to ``mean=0``, ``stdev=0.02``.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is ``(nc) x 64 x 64``\n",
    "            nn.Conv1d(nc, ndf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf) x 32 x 32``\n",
    "            nn.Conv1d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*2) x 16 x 16``\n",
    "            nn.Conv1d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*4) x 8 x 8``\n",
    "            nn.Conv1d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*8) x 4 x 4``\n",
    "            nn.Conv1d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv1d(1, 64, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,), bias=False)\n",
      "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv1d(512, 1, kernel_size=(4,), stride=(1,), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-GPU if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly initialize all weights\n",
    "# like this: ``to mean=0, stdev=0.2``.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ``BCELoss`` function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[tensor([[ 4.3200,  4.9200, 12.2000,  0.1300],\n",
      "        [ 8.3600, 10.0400, 13.9400,  2.6500],\n",
      "        [ 8.8500,  4.8600,  8.8600,  0.3500],\n",
      "        [ 7.8400,  8.8000, 13.1600,  7.3500],\n",
      "        [ 4.8800,  6.7300,  9.7600,  6.5400],\n",
      "        [ 4.1000,  4.7700,  8.8000,  0.1300],\n",
      "        [ 6.1600,  7.1500,  8.2500,  4.5900],\n",
      "        [ 5.1600,  7.0200, 12.8100,  3.3500],\n",
      "        [ 5.2600,  5.8400,  7.0900,  4.4400],\n",
      "        [ 4.3300,  4.0500, 11.2400,  2.0900],\n",
      "        [ 8.6500,  6.6000,  8.9700,  1.9400],\n",
      "        [ 3.6900,  7.4900,  6.6800,  6.1800],\n",
      "        [ 4.4000,  4.7100,  8.6200,  4.2200],\n",
      "        [ 6.4900, 10.9700, 12.5800,  5.4800],\n",
      "        [ 5.8400,  4.4900,  8.4200,  2.0700],\n",
      "        [ 4.9200,  6.2400, 11.7700,  1.8400],\n",
      "        [ 5.9700,  6.4400,  8.5200,  2.7100],\n",
      "        [ 5.1200,  7.1500,  9.9000,  1.3100],\n",
      "        [ 7.3300,  4.3000, 11.2900,  3.5100],\n",
      "        [ 4.2600,  5.0300,  6.3400,  5.1700],\n",
      "        [ 4.6400,  6.5600,  9.1700,  1.7600],\n",
      "        [ 5.0100,  3.7800,  9.0200,  3.0400],\n",
      "        [ 4.1400,  8.0400, 10.6600,  4.7800],\n",
      "        [ 5.8600,  5.5100,  9.5000,  1.3200],\n",
      "        [ 7.1400,  6.3600, 13.9400,  0.0000],\n",
      "        [ 5.0600, 10.2800, 10.0700,  8.1800],\n",
      "        [ 3.8200,  7.4300, 11.9900,  3.9900],\n",
      "        [ 4.8500,  6.4100,  9.0900,  1.1800],\n",
      "        [ 3.6600,  4.7400,  9.4100,  3.8700],\n",
      "        [ 7.7800,  7.9300,  8.2500,  4.6300],\n",
      "        [ 6.7600,  8.8600, 12.8300,  4.3300],\n",
      "        [ 6.1000,  5.2700,  9.0600,  0.8600],\n",
      "        [ 6.4100,  6.9000,  8.9200,  2.1700],\n",
      "        [ 3.9200,  9.8700, 12.0500,  5.5000],\n",
      "        [ 5.2900,  4.4200,  7.5000,  3.1100],\n",
      "        [ 3.5900,  3.3000,  9.8600,  0.0000],\n",
      "        [ 4.5500,  6.1900, 11.4500,  2.3000],\n",
      "        [ 4.7900,  8.1900,  8.0700,  6.4000],\n",
      "        [ 3.8500,  3.8600,  5.7900,  2.4300],\n",
      "        [ 4.0100,  6.5300, 10.3800,  2.8300],\n",
      "        [ 5.1000,  5.4200,  9.8500,  3.1800],\n",
      "        [ 4.8200,  6.8400,  9.3300,  2.8500],\n",
      "        [ 4.0500,  5.6000,  8.3300,  1.7100],\n",
      "        [ 5.6800,  8.4000, 10.1300,  4.7100],\n",
      "        [ 4.3200,  7.0400,  9.5600,  4.2600],\n",
      "        [ 5.4000,  3.9600,  5.8600,  0.5700],\n",
      "        [ 6.8000,  7.0200,  8.8100,  6.0100],\n",
      "        [ 5.4400,  3.6300, 10.5800,  1.0400],\n",
      "        [ 3.6600,  8.5100, 11.3400,  3.8900],\n",
      "        [ 4.7600,  8.2800, 10.6800,  2.4600],\n",
      "        [ 5.5100,  8.1200,  8.6700,  3.6300],\n",
      "        [ 4.0100,  4.8100, 11.9500,  2.9600],\n",
      "        [ 4.4700,  6.0500, 11.6500,  1.0600],\n",
      "        [ 3.9700,  2.9900,  7.9500,  4.3700],\n",
      "        [ 4.7600,  8.0500, 12.2000,  3.8100],\n",
      "        [ 4.8700,  4.2400, 10.9700,  3.3900],\n",
      "        [ 5.0500,  8.5200, 11.8900,  4.8300],\n",
      "        [ 6.2800,  5.0300, 11.3300,  2.0500],\n",
      "        [ 3.9400,  5.2500, 10.7700,  5.4800],\n",
      "        [ 5.5000, 10.1500,  9.3300,  3.5000],\n",
      "        [ 5.2900,  4.0500,  8.4900,  8.2400],\n",
      "        [ 5.0600,  6.8500, 10.3100,  1.6000],\n",
      "        [ 5.0800,  3.8800,  5.7300,  4.7100],\n",
      "        [ 5.2100,  4.9300, 14.0300,  2.0400],\n",
      "        [ 4.5500,  6.0900,  8.6600,  2.0000],\n",
      "        [ 3.7000,  9.4100,  5.1000,  3.7100],\n",
      "        [ 5.3500,  4.7000, 13.6100,  0.8100],\n",
      "        [ 7.5200,  3.9600,  3.9700,  0.2700],\n",
      "        [ 7.5700,  4.6000, 10.7500,  0.0000],\n",
      "        [ 5.2300,  6.0900, 11.1800,  0.5200],\n",
      "        [ 4.4700,  5.4800, 11.7800,  4.4000],\n",
      "        [ 3.4000,  5.6600,  9.4900,  1.9000],\n",
      "        [ 4.2000,  6.2300, 11.6600,  4.3600],\n",
      "        [ 7.0500,  4.4700, 12.4200,  3.0300],\n",
      "        [ 5.2900,  9.0000,  8.6000,  4.3300],\n",
      "        [ 5.1500,  5.7500, 10.9600,  3.6000],\n",
      "        [ 8.7400,  3.9600,  6.3900,  0.0000],\n",
      "        [ 4.1000,  3.6000,  6.4300,  1.0800],\n",
      "        [ 6.6900,  8.2100, 11.0100,  3.6900],\n",
      "        [ 6.3600,  7.4200, 10.9500,  3.4100],\n",
      "        [ 4.3700,  6.9400,  9.5200,  1.6400],\n",
      "        [ 5.2300,  5.3800,  9.8100,  4.5000],\n",
      "        [ 3.9400,  7.8500,  8.8900,  2.8600],\n",
      "        [ 7.6300,  6.8300,  9.5800,  3.8000],\n",
      "        [ 7.0400,  7.3700, 11.0100,  5.9600],\n",
      "        [ 6.2600,  7.3600,  9.2900,  2.1500],\n",
      "        [ 5.4000,  4.9800,  8.3200,  2.6200],\n",
      "        [ 4.8700,  8.1400, 10.4300,  1.3300],\n",
      "        [ 4.9100,  7.0800,  8.4400,  3.1500],\n",
      "        [ 3.9400,  5.8000, 10.4800,  7.5400],\n",
      "        [ 4.0600,  4.7500, 12.0000,  2.8600],\n",
      "        [ 7.6000, 10.3200, 11.3400,  6.1800],\n",
      "        [ 6.0200,  6.8900,  6.0200,  1.6600],\n",
      "        [ 6.8600,  6.1300, 11.6300,  1.2200],\n",
      "        [ 4.8000,  6.5300, 10.0600,  3.5100],\n",
      "        [ 3.5200,  7.4800,  9.0100,  2.3100],\n",
      "        [ 4.4300,  4.8600, 11.1800,  2.7400],\n",
      "        [ 4.5100,  7.4900, 10.6800,  5.3700],\n",
      "        [ 4.8100,  4.7600,  9.7100,  0.6200],\n",
      "        [ 4.1400,  5.4500,  7.4800,  2.6200]]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 1, 4], expected input[1, 100, 4] to have 1 channels, but got 100 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((b_size,), real_label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Forward pass real batch through D\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnetD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_cpu\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate loss on all-real batch\u001b[39;00m\n\u001b[0;32m     29\u001b[0m errD_real \u001b[38;5;241m=\u001b[39m criterion(output, label)\n",
      "File \u001b[1;32mc:\\Users\\sducr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sducr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[131], line 27\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sducr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sducr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sducr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sducr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sducr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sducr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sducr\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 1, 4], expected input[1, 100, 4] to have 1 channels, but got 100 channels instead"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for data in dataloader:\n",
    "\n",
    "        print(data)\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        # if i % 50 == 0:\n",
    "        #     print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "        #           % (epoch, num_epochs, i, len(dataloader),\n",
    "        #              errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        # if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "        #     with torch.no_grad():\n",
    "        #         fake = netG(fixed_noise).detach().cpu()\n",
    "        #     img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
